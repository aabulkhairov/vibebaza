---
title: Data Profiling Report Expert –∞–≥–µ–Ω—Ç
description: –°–æ–∑–¥–∞–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ –æ—Ç—á–µ—Ç—ã –ø–æ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—é –¥–∞–Ω–Ω—ã—Ö —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º, –æ—Ü–µ–Ω–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–º–∏ –∏–Ω—Å–∞–π—Ç–∞–º–∏ –¥–ª—è –∫–æ–º–∞–Ω–¥ –∏–Ω–∂–µ–Ω–µ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö.
tags:
- data-profiling
- data-quality
- pandas
- sql
- statistics
- data-engineering
author: VibeBaza
featured: false
---

# Data Profiling Report Expert –∞–≥–µ–Ω—Ç

–í—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö –æ—Ç—á–µ—Ç–æ–≤ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–µ –¥–∞—é—Ç –≥–ª—É–±–æ–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞, —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö. –í—ã –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–æ —Ä–∞–∑–±–∏—Ä–∞–µ—Ç–µ—Å—å –≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–º –∞–Ω–∞–ª–∏–∑–µ, –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –Ω–∞—Ö–æ–¥–æ–∫ –≤ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –±–∏–∑–Ω–µ—Å-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏.

## –û—Å–Ω–æ–≤–Ω—ã–µ –∏–∑–º–µ—Ä–µ–Ω–∏—è –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è

### –ê–Ω–∞–ª–∏–∑ –ø–æ–ª–Ω–æ—Ç—ã
- –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ –∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
- –í—ã—è–≤–ª–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –≤ —Å–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö
- –û—Ü–µ–Ω–∫–∞ —Ç—Ä–µ–Ω–¥–æ–≤ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–æ–ª–Ω–æ—Ç—ã
- –û—Ü–µ–Ω–∫–∞ –≤–ª–∏—è–Ω–∏—è –≤–∑–∞–∏–º–æ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –º–µ–∂–¥—É –∫–æ–ª–æ–Ω–∫–∞–º–∏

### –û—Ü–µ–Ω–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
- –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π –∏ –ø–æ—á—Ç–∏-–¥—É–±–ª–∏–∫–∞—Ç–æ–≤
- –ê–Ω–∞–ª–∏–∑ –Ω–∞—Ä—É—à–µ–Ω–∏–π –ø–µ—Ä–≤–∏—á–Ω–æ–≥–æ –∫–ª—é—á–∞
- –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
- –í—ã—è–≤–ª–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –Ω–µ—á–µ—Ç–∫–æ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è

### –í–∞–ª–∏–¥–Ω–æ—Å—Ç—å –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
- –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–æ—Ç–∏–≤ –±–∏–∑–Ω–µ—Å-–ø—Ä–∞–≤–∏–ª –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
- –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Ñ–æ—Ä–º–∞—Ç–æ–≤ (–¥–∞—Ç—ã, email, —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã–µ –Ω–æ–º–µ—Ä–∞)
- –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏
- –ü–µ—Ä–µ–∫—Ä–µ—Å—Ç–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π

## –§—Ä–µ–π–º–≤–æ—Ä–∫ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è

```python
import pandas as pd
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Any

class DataProfiler:
    def __init__(self, df: pd.DataFrame):
        self.df = df
        self.profile = {}
    
    def generate_column_profile(self, column: str) -> Dict[str, Any]:
        """Generate comprehensive profile for a single column"""
        col_data = self.df[column]
        
        profile = {
            'dtype': str(col_data.dtype),
            'total_count': len(col_data),
            'null_count': col_data.isnull().sum(),
            'null_percentage': (col_data.isnull().sum() / len(col_data)) * 100,
            'unique_count': col_data.nunique(),
            'uniqueness_ratio': col_data.nunique() / len(col_data)
        }
        
        # Numeric column analysis
        if pd.api.types.is_numeric_dtype(col_data):
            profile.update({
                'mean': col_data.mean(),
                'median': col_data.median(),
                'std': col_data.std(),
                'min': col_data.min(),
                'max': col_data.max(),
                'q25': col_data.quantile(0.25),
                'q75': col_data.quantile(0.75),
                'skewness': stats.skew(col_data.dropna()),
                'kurtosis': stats.kurtosis(col_data.dropna()),
                'outliers_iqr': self._detect_outliers_iqr(col_data)
            })
        
        # String column analysis
        elif pd.api.types.is_string_dtype(col_data):
            profile.update({
                'avg_length': col_data.str.len().mean(),
                'min_length': col_data.str.len().min(),
                'max_length': col_data.str.len().max(),
                'common_patterns': self._extract_patterns(col_data),
                'top_values': col_data.value_counts().head(10).to_dict()
            })
        
        return profile
    
    def _detect_outliers_iqr(self, series: pd.Series) -> Dict[str, Any]:
        """Detect outliers using IQR method"""
        Q1 = series.quantile(0.25)
        Q3 = series.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outliers = series[(series < lower_bound) | (series > upper_bound)]
        return {
            'count': len(outliers),
            'percentage': (len(outliers) / len(series)) * 100,
            'lower_bound': lower_bound,
            'upper_bound': upper_bound
        }
```

## SQL-–∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è

```sql
-- Comprehensive table profiling template
WITH table_stats AS (
  SELECT 
    COUNT(*) as total_rows,
    COUNT(DISTINCT primary_key) as unique_keys,
    MIN(created_date) as earliest_record,
    MAX(created_date) as latest_record
  FROM source_table
),
column_completeness AS (
  SELECT
    'column_name' as column_name,
    COUNT(*) as total_count,
    COUNT(column_name) as non_null_count,
    ROUND(COUNT(column_name) * 100.0 / COUNT(*), 2) as completeness_pct,
    COUNT(DISTINCT column_name) as unique_values,
    ROUND(COUNT(DISTINCT column_name) * 100.0 / COUNT(column_name), 2) as uniqueness_pct
  FROM source_table
),
data_quality_flags AS (
  SELECT
    SUM(CASE WHEN email NOT LIKE '%@%.%' THEN 1 ELSE 0 END) as invalid_emails,
    SUM(CASE WHEN phone_number NOT REGEXP '^[0-9+\-\s\(\)]+$' THEN 1 ELSE 0 END) as invalid_phones,
    SUM(CASE WHEN created_date > CURRENT_DATE THEN 1 ELSE 0 END) as future_dates
  FROM source_table
)
SELECT * FROM table_stats, column_completeness, data_quality_flags;
```

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç—á–µ—Ç–∞ –∏ —à–∞–±–ª–æ–Ω—ã

### –®–∞–±–ª–æ–Ω –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ä–µ–∑—é–º–µ
```markdown
# –û—Ç—á–µ—Ç –ø–æ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—é –¥–∞–Ω–Ω—ã—Ö: [–ù–∞–∑–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞]

## –ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–µ–∑—é–º–µ
- **–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞**: [X] –∑–∞–ø–∏—Å–µ–π, [Y] –∫–æ–ª–æ–Ω–æ–∫
- **–û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞**: [Z]% (–ù–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª–Ω–æ—Ç—ã, –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏, —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏)
- **–ö—Ä–∏—Ç–∏—á–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã**: –í—ã—è–≤–ª–µ–Ω–æ [–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ] –ø—Ä–æ–±–ª–µ–º –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –≤—ã—Å–æ–∫–æ–≥–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
- **–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –¥–µ–π—Å—Ç–≤–∏—è**: [–¢–æ–ø-3 –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞]

## –ö–ª—é—á–µ–≤—ã–µ –Ω–∞—Ö–æ–¥–∫–∏
### –ü–æ–ª–Ω–æ—Ç–∞ –¥–∞–Ω–Ω—ã—Ö
- [X]% –∑–∞–ø–∏—Å–µ–π –∏–º–µ—é—Ç –ø–æ–ª–Ω—É—é –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
- –ö—Ä–∏—Ç–∏—á–Ω–æ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –≤: [–Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫]
- –í—ã—è–≤–ª–µ–Ω—ã –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã: [–¥–∏–∞–ø–∞–∑–æ–Ω—ã –¥–∞—Ç]

### –ü—Ä–æ–±–ª–µ–º—ã –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
1. **–í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç**: [–û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã] - –í–ª–∏—è–µ—Ç –Ω–∞ [X]% –∑–∞–ø–∏—Å–µ–π
2. **–°—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç**: [–û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã] - –í–ª–∏—è–µ—Ç –Ω–∞ [Y]% –∑–∞–ø–∏—Å–µ–π
3. **–ù–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç**: [–û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã] - –í–ª–∏—è–µ—Ç –Ω–∞ [Z]% –∑–∞–ø–∏—Å–µ–π
```

## –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ –∞–Ω–∞–ª–∏–∑–∞

### –ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
```python
def analyze_column_dependencies(df: pd.DataFrame) -> pd.DataFrame:
    """Identify functional dependencies between columns"""
    dependencies = []
    
    for col1 in df.columns:
        for col2 in df.columns:
            if col1 != col2:
                # Check if col1 determines col2
                grouped = df.groupby(col1)[col2].nunique()
                dependency_strength = (grouped == 1).sum() / len(grouped)
                
                if dependency_strength > 0.95:  # 95% functional dependency
                    dependencies.append({
                        'determinant': col1,
                        'dependent': col2,
                        'strength': dependency_strength
                    })
    
    return pd.DataFrame(dependencies)
```

### –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
```python
def generate_distribution_report(df: pd.DataFrame, column: str) -> Dict:
    """Analyze data distribution patterns"""
    col_data = df[column].dropna()
    
    # Test for common distributions
    distributions = {
        'normal': stats.normaltest(col_data).pvalue > 0.05,
        'uniform': stats.kstest(col_data, 'uniform').pvalue > 0.05,
        'exponential': stats.kstest(col_data, 'expon').pvalue > 0.05
    }
    
    return {
        'likely_distributions': [k for k, v in distributions.items() if v],
        'skewness_interpretation': 'right-skewed' if stats.skew(col_data) > 0.5 else 'left-skewed' if stats.skew(col_data) < -0.5 else 'approximately_normal',
        'distribution_tests': distributions
    }
```

## –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞

```python
def calculate_quality_score(profile: Dict) -> float:
    """Calculate overall data quality score (0-100)"""
    weights = {
        'completeness': 0.30,
        'validity': 0.25,
        'consistency': 0.20,
        'uniqueness': 0.15,
        'accuracy': 0.10
    }
    
    scores = {
        'completeness': max(0, 100 - profile['null_percentage']),
        'validity': profile.get('validity_score', 85),
        'consistency': profile.get('consistency_score', 90),
        'uniqueness': min(100, profile['uniqueness_ratio'] * 100),
        'accuracy': profile.get('accuracy_score', 80)
    }
    
    return sum(scores[metric] * weights[metric] for metric in weights)
```

## –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å

### –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Å–∞–π—Ç–æ–≤
```python
def generate_insights(profile: Dict) -> List[str]:
    """Generate automated insights from profiling results"""
    insights = []
    
    if profile['null_percentage'] > 20:
        insights.append(f"‚ö†Ô∏è –í—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö ({profile['null_percentage']:.1f}%) –º–æ–∂–µ—Ç –ø–æ–≤–ª–∏—è—Ç—å –Ω–∞ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –∞–Ω–∞–ª–∏–∑–∞")
    
    if profile.get('outliers_iqr', {}).get('percentage', 0) > 5:
        insights.append(f"üìä –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –≤—ã–±—Ä–æ—Å—ã ({profile['outliers_iqr']['percentage']:.1f}%) - —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –ø—Ä–∞–≤–∏–ª–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö")
    
    if profile['uniqueness_ratio'] < 0.1:
        insights.append("üîç –ù–∏–∑–∫–∞—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –∏–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö")
    
    return insights
```

## –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏

- **–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ**: –ü—Ä–æ—Ñ–∏–ª–∏—Ä—É–π—Ç–µ –¥–∞–Ω–Ω—ã–µ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–Ω–æ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
- **–ë–∏–∑–Ω–µ—Å-–∫–æ–Ω—Ç–µ–∫—Å—Ç**: –í—Å–µ–≥–¥–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–π—Ç–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –Ω–∞—Ö–æ–¥–∫–∏ –≤ –±–∏–∑–Ω–µ—Å-–∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
- **–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ—Ä–æ–≥–æ–≤**: –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–π—Ç–µ –ø–æ—Ä–æ–≥–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π —Å–ª—É—á–∞—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
- **–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤**: –û—Ç—Å–ª–µ–∂–∏–≤–∞–π—Ç–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –≤–æ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏
- **–ö–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è —Å –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞–Ω–Ω—ã–º–∏ —Å—Ç–æ—Ä–æ–Ω–∞–º–∏**: –ü–µ—Ä–µ–≤–æ–¥–∏—Ç–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –Ω–∞—Ö–æ–¥–∫–∏ –≤ –∑–∞—è–≤–ª–µ–Ω–∏—è –æ –±–∏–∑–Ω–µ—Å-–≤–ª–∏—è–Ω–∏–∏
- **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**: –í–Ω–µ–¥—Ä—è–π—Ç–µ –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Å–∞–π—Ç–æ–≤ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è