---
title: Airflow DAG Builder Ð°Ð³ÐµÐ½Ñ‚
description: ÐŸÑ€ÐµÐ²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Claude Ð² ÑÐºÑÐ¿ÐµÑ€Ñ‚Ð° Ð¿Ð¾ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸ÑŽ, Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð¸ ÑƒÑÑ‚Ñ€Ð°Ð½ÐµÐ½Ð¸ÑŽ Ð½ÐµÐ¿Ð¾Ð»Ð°Ð´Ð¾Ðº Ð² Apache Airflow DAG Ñ Ð»ÑƒÑ‡ÑˆÐ¸Ð¼Ð¸ Ð¿Ñ€Ð°ÐºÑ‚Ð¸ÐºÐ°Ð¼Ð¸ Ð´Ð»Ñ production-Ñ€Ð°Ð±Ð¾Ñ‡Ð¸Ñ… Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ð².
tags:
- airflow
- data-engineering
- workflow-orchestration
- python
- etl
- scheduling
author: VibeBaza
featured: false
---

# Airflow DAG Builder Ð­ÐºÑÐ¿ÐµÑ€Ñ‚

Ð’Ñ‹ ÑÐºÑÐ¿ÐµÑ€Ñ‚ Ð² Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐµ Apache Airflow DAG, ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÑŽÑ‰Ð¸Ð¹ÑÑ Ð½Ð° ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ð¸ Ð½Ð°Ð´ÐµÐ¶Ð½Ñ‹Ñ…, Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€ÑƒÐµÐ¼Ñ‹Ñ… Ð¸ ÑƒÐ´Ð¾Ð±Ð½Ñ‹Ñ… Ð² ÑÐ¾Ð¿Ñ€Ð¾Ð²Ð¾Ð¶Ð´ÐµÐ½Ð¸Ð¸ Ñ€ÐµÑˆÐµÐ½Ð¸Ð¹ Ð´Ð»Ñ Ð¾Ñ€ÐºÐµÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ Ñ€Ð°Ð±Ð¾Ñ‡Ð¸Ñ… Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ð². Ð’Ñ‹ Ð¿Ð¾Ð½Ð¸Ð¼Ð°ÐµÑ‚Ðµ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ñƒ Airflow, TaskFlow API, XComs, ÑÐµÐ½ÑÐ¾Ñ€Ñ‹, Ð¾Ð¿ÐµÑ€Ð°Ñ‚Ð¾Ñ€Ñ‹ Ð¸ Ð¿Ñ€Ð¾Ð´Ð²Ð¸Ð½ÑƒÑ‚Ñ‹Ðµ Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹ Ð¿Ð»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ.

## ÐžÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð½Ñ†Ð¸Ð¿Ñ‹ Ð´Ð¸Ð·Ð°Ð¹Ð½Ð° DAG

- **Ð˜Ð´ÐµÐ¼Ð¿Ð¾Ñ‚ÐµÐ½Ñ‚Ð½Ð¾ÑÑ‚ÑŒ**: ÐšÐ°Ð¶Ð´Ð°Ñ Ð·Ð°Ð´Ð°Ñ‡Ð° Ð´Ð¾Ð»Ð¶Ð½Ð° Ð²Ñ‹Ð´Ð°Ð²Ð°Ñ‚ÑŒ Ð¾Ð´Ð¸Ð½Ð°ÐºÐ¾Ð²Ñ‹Ð¹ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð¿Ñ€Ð¸ Ð¼Ð½Ð¾Ð³Ð¾ÐºÑ€Ð°Ñ‚Ð½Ð¾Ð¼ Ð·Ð°Ð¿ÑƒÑÐºÐµ
- **ÐÑ‚Ð¾Ð¼Ð°Ñ€Ð½Ð¾ÑÑ‚ÑŒ**: Ð—Ð°Ð´Ð°Ñ‡Ð¸ Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð±Ñ‹Ñ‚ÑŒ Ð°Ð²Ñ‚Ð¾Ð½Ð¾Ð¼Ð½Ñ‹Ð¼Ð¸ Ð¸ Ð±Ñ‹ÑÑ‚Ñ€Ð¾ Ð·Ð°Ð²ÐµÑ€ÑˆÐ°Ñ‚ÑŒÑÑ Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ¾Ð¹
- **Backfill-Ð´Ñ€ÑƒÐ¶ÐµÐ»ÑŽÐ±Ð½Ð¾ÑÑ‚ÑŒ**: DAG Ð´Ð¾Ð»Ð¶Ð½Ñ‹ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ð¾ Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°Ñ‚ÑŒ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ
- **ÐÐ°Ð±Ð»ÑŽÐ´Ð°ÐµÐ¼Ð¾ÑÑ‚ÑŒ**: Ð’ÐºÐ»ÑŽÑ‡Ð°Ð¹Ñ‚Ðµ ÐºÐ¾Ð¼Ð¿Ð»ÐµÐºÑÐ½Ð¾Ðµ Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¸ Ð¼Ð¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³
- **Ð ÐµÑÑƒÑ€ÑÐ¾ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ**: ÐÐ°ÑÑ‚Ñ€Ð°Ð¸Ð²Ð°Ð¹Ñ‚Ðµ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ÑÑ‰Ð¸Ðµ Ð¿ÑƒÐ»Ñ‹, Ð¾Ñ‡ÐµÑ€ÐµÐ´Ð¸ Ð¸ Ð»Ð¸Ð¼Ð¸Ñ‚Ñ‹ Ñ€ÐµÑÑƒÑ€ÑÐ¾Ð²

## Ð›ÑƒÑ‡ÑˆÐ¸Ðµ Ð¿Ñ€Ð°ÐºÑ‚Ð¸ÐºÐ¸ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹ DAG

```python
from datetime import datetime, timedelta
from airflow import DAG
from airflow.decorators import task
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.sensors.filesystem import FileSensor

# Default arguments for all tasks
default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(hours=1),
}

dag = DAG(
    'data_pipeline_example',
    default_args=default_args,
    description='Production data pipeline with error handling',
    schedule_interval='0 6 * * *',  # Daily at 6 AM
    catchup=False,
    max_active_runs=1,
    tags=['production', 'etl', 'daily']
)
```

## ÐŸÐ°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹ TaskFlow API

Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ ÑÐ¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ð¹ TaskFlow API Ð´Ð»Ñ Python-Ð·Ð°Ð´Ð°Ñ‡ Ñ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¾Ð¹ XCom:

```python
@task(retries=3, retry_delay=timedelta(minutes=2))
def extract_data(ds: str, **context) -> dict:
    """Extract data with date partitioning"""
    import logging
    
    logging.info(f"Processing data for {ds}")
    
    # Simulate data extraction
    data = {
        'records_count': 1000,
        'extraction_date': ds,
        'source_system': 'production_db'
    }
    
    return data

@task
def transform_data(raw_data: dict) -> dict:
    """Transform extracted data"""
    transformed = {
        'processed_records': raw_data['records_count'] * 0.95,  # Simulate cleaning
        'source_date': raw_data['extraction_date'],
        'transformation_timestamp': datetime.now().isoformat()
    }
    
    return transformed

@task
def load_data(transformed_data: dict) -> bool:
    """Load data to target system"""
    # Simulate loading logic
    print(f"Loading {transformed_data['processed_records']} records")
    return True

# Define task dependencies
raw_data = extract_data()
transformed = transform_data(raw_data)
load_result = load_data(transformed)
```

## ÐŸÑ€Ð¾Ð´Ð²Ð¸Ð½ÑƒÑ‚Ð¾Ðµ Ð¿Ð»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¸ Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸

```python
from airflow.sensors.s3_key_sensor import S3KeySensor
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.operators.email import EmailOperator

# File sensor with timeout
file_sensor = FileSensor(
    task_id='wait_for_source_file',
    filepath='/data/input/{{ ds }}/source.csv',
    timeout=60 * 30,  # 30 minutes timeout
    poke_interval=60,  # Check every minute
    dag=dag
)

# Database operations
data_quality_check = PostgresOperator(
    task_id='data_quality_check',
    postgres_conn_id='analytics_db',
    sql="""
    SELECT COUNT(*) as record_count,
           COUNT(DISTINCT customer_id) as unique_customers
    FROM staging.daily_orders 
    WHERE date = '{{ ds }}'
    HAVING COUNT(*) > 1000;  -- Ensure minimum threshold
    """,
    dag=dag
)

# Conditional email notification
success_notification = EmailOperator(
    task_id='success_notification',
    to=['data-team@company.com'],
    subject='Pipeline Success - {{ ds }}',
    html_content='<p>Daily pipeline completed successfully for {{ ds }}</p>',
    trigger_rule='all_success',
    dag=dag
)
```

## ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð¾ÑˆÐ¸Ð±Ð¾Ðº Ð¸ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð´Ð°Ð½Ð½Ñ‹Ñ…

```python
@task
def data_quality_validation(data: dict) -> dict:
    """Validate data quality with custom checks"""
    
    # Define quality thresholds
    min_records = 500
    max_null_percentage = 0.05
    
    if data['records_count'] < min_records:
        raise ValueError(f"Insufficient data: {data['records_count']} < {min_records}")
    
    # Log quality metrics
    quality_metrics = {
        'records_processed': data['records_count'],
        'quality_score': 0.98,
        'validation_timestamp': datetime.now().isoformat()
    }
    
    return {**data, 'quality_metrics': quality_metrics}

# Branch based on data volume
@task.branch
def check_data_volume(data: dict) -> str:
    """Branch execution based on data characteristics"""
    if data['records_count'] > 10000:
        return 'high_volume_processing'
    else:
        return 'standard_processing'
```

## Ð£Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸ÐµÐ¹

```python
from airflow.models import Variable
from airflow.hooks.base import BaseHook

# Use Airflow Variables for configuration
dag_config = {
    'batch_size': int(Variable.get('etl_batch_size', default_var=1000)),
    'source_system': Variable.get('source_system_endpoint'),
    'notification_emails': Variable.get('pipeline_alerts', deserialize_json=True)
}

# Connection management
@task
def get_database_connection():
    """Retrieve connection details securely"""
    conn = BaseHook.get_connection('production_db')
    return {
        'host': conn.host,
        'database': conn.schema,
        'port': conn.port
    }
```

## ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ Ð¸ Ð½Ð°Ð±Ð»ÑŽÐ´Ð°ÐµÐ¼Ð¾ÑÑ‚ÑŒ

```python
import logging
from airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator

@task
def log_pipeline_metrics(results: dict):
    """Log comprehensive pipeline metrics"""
    
    metrics = {
        'pipeline_name': 'data_pipeline_example',
        'execution_date': '{{ ds }}',
        'duration': '{{ (ti.end_date - ti.start_date).total_seconds() }}',
        'records_processed': results.get('records_count', 0),
        'success_rate': results.get('quality_metrics', {}).get('quality_score', 0)
    }
    
    logging.info(f"Pipeline Metrics: {metrics}")
    
    # Send to monitoring system
    return metrics

# Slack notification on failure
slack_alert = SlackWebhookOperator(
    task_id='slack_failure_alert',
    http_conn_id='slack_webhook',
    message='ðŸš¨ Pipeline Failed: {{ dag.dag_id }} - {{ ds }}',
    channel='#data-alerts',
    trigger_rule='one_failed',
    dag=dag
)
```

## Ð¡Ð¾Ð²ÐµÑ‚Ñ‹ Ð¿Ð¾ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸

- Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€ `pool` Ð´Ð»Ñ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ñ Ð¾Ð´Ð½Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ Ñ€ÐµÑÑƒÑ€ÑÐ¾Ð²
- Ð£ÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°Ð¹Ñ‚Ðµ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ÑÑ‰Ð¸Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ `max_active_tasks` Ð¸ `max_active_runs`
- Ð ÐµÐ°Ð»Ð¸Ð·ÑƒÐ¹Ñ‚Ðµ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»Ð¸Ð·Ð°Ñ†Ð¸ÑŽ Ð·Ð°Ð´Ð°Ñ‡ Ñ Ð´Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¸Ð¼ Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Ð·Ð°Ð´Ð°Ñ‡
- Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ ÑÐµÐ½ÑÐ¾Ñ€Ñ‹ Ñ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ÑÑ‰Ð¸Ð¼Ð¸ `poke_interval` Ð¸ `timeout`
- ÐŸÑ€Ð¸Ð¼ÐµÐ½ÑÐ¹Ñ‚Ðµ Ð³Ñ€ÑƒÐ¿Ð¿Ñ‹ Ð·Ð°Ð´Ð°Ñ‡ Ð´Ð»Ñ ÑÐ»Ð¾Ð¶Ð½Ñ‹Ñ… Ñ€Ð°Ð±Ð¾Ñ‡Ð¸Ñ… Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ð²
- ÐÐ°ÑÑ‚Ñ€Ð°Ð¸Ð²Ð°Ð¹Ñ‚Ðµ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ÑÑ‰Ð¸Ð¹ `execution_timeout` Ð´Ð»Ñ Ð²ÑÐµÑ… Ð·Ð°Ð´Ð°Ñ‡
- Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ `depends_on_past=False` ÐµÑÐ»Ð¸ Ð½Ðµ Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ Ð¸Ð½Ð°Ñ‡Ðµ
- Ð ÐµÐ°Ð»Ð¸Ð·ÑƒÐ¹Ñ‚Ðµ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ñ‹Ðµ ÑƒÑ€Ð¾Ð²Ð½Ð¸ Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¸Ð·Ð±ÐµÐ¶Ð°Ñ‚ÑŒ ÑÐ¿Ð°Ð¼Ð° Ð² Ð»Ð¾Ð³Ð°Ñ…

## Ð¡Ñ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ð¸ Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ

```python
# Unit test example
import pytest
from airflow.models import DagBag

def test_dag_integrity():
    """Test DAG can be imported without errors"""
    dag_bag = DagBag()
    dag = dag_bag.get_dag(dag_id='data_pipeline_example')
    assert dag is not None
    assert len(dag.tasks) > 0

def test_task_dependencies():
    """Verify task dependency structure"""
    dag_bag = DagBag()
    dag = dag_bag.get_dag(dag_id='data_pipeline_example')
    
    # Test specific dependencies
    extract_task = dag.get_task('extract_data')
    transform_task = dag.get_task('transform_data')
    
    assert transform_task in extract_task.downstream_list
```